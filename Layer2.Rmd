## 第2層 層の追加

この章では、層を1つ追加して3層にします。 

引き続きKaggleの手書き文字認識をします。  
http://www.kaggle.com/c/digit-recognizer

とりあえずデータを読み込みます。
```{r}
setwd("/Users/KeiHarada/Documents/digit")
# read data
traindata <- read.csv("ORG/train.csv")
testdata <- read.csv("ORG/test.csv")
train_label <- traindata$label[1:28000]
valid_label <- traindata$label[28001:42000]
```

ここでは28*28のまま扱います。

```{r}
train_mat <- as.matrix(traindata[1:28000,-1])
# check
image(matrix(train_mat[1,],ncol=28))
```


##### 復習
まずは復習を兼ねて、4×4のエリアの平均値をとって、7×7の画像とし、そこからニューラルネットワークで学習してみましょう。
ここでは平均値が127より大きいかどうかで0/1に変換します。
ちょっと工夫して行列の演算を利用して変換します。

```{r}
meanMat <- matrix(0.0,nrow=28*28,ncol=49)
for (i in seq(7)){
  for (j in seq(7)){
    tempmat <- matrix(0.0,nrow=28,ncol=28)
    tempmat[((4*i-3):(4*i)),((4*j-3):(4*j))] <- 1.0/16.0
    meanMat[,7*(j-1)+i] <- as.numeric(tempmat)
  }
}

train49_mat_mean <- sign(train_mat %*% meanMat - 127)
```

うまくできてるかチェックしましょう。
```{r}
image(matrix(train49_mat_mean[1,],nrow=7))
```

次に、学習のために正解を用意します。

```{r}
answer_mat <- matrix(0,nrow=28000,ncol=10)
for (i in seq(10)){
  answer_mat[train_label==(i-1),i] <- 1
}
```

では、学習させてみましょう。

```{r}
# learn weights
W <- matrix(0.0,nrow=10,ncol=49)
intercept <- rep(0.0,length=10)

# learn rate
eta <- 0.001
for (i in seq(28000)){
    # feed forward
    output <- 1 / (1 + exp(-1*(W %*% train49_mat_mean[i,] + intercept)))
    
    # back propagation
    W <- W + eta * (answer_mat[i,] - output) %*% t(train49_mat_mean[i,])
    intercept <- intercept + eta * (answer_mat[i,] - output)
    
}
```

出力の計算

```{r}
output_mat <- 1 / (1 + exp(-1*(train49_mat_mean %*% t(W) + matrix(1,nrow=28000,ncol=1) %*% matrix(intercept,ncol=10))))
trainres <- max.col(as.matrix(output_mat)) - 1
```

Confusion Matrix

```{r}
table(trainres,train_label)
```

Categorization Accuracy

```{r}
mean(trainres == train_label)
```

最大値よりも精度が落ちてますね。


##### 両方ともニューラルネットに

ここで、さきほどの平均を取る作業もニューラルネットの一部とみなします。

```{r}
# learn weights
W1 <- t(meanMat)
intercept1 <- rep(-127.0,length=49)

W2 <- matrix(0.0,nrow=10,ncol=49)
intercept2 <- rep(0.0,length=10)

# learn rate
eta1 <- 0.001
eta2 <- 0.001

for (i in seq(28000)){
  # feed forward
  output1 <- 1 / (1 + exp(-1*(W1 %*% train_mat[i,] + intercept1)))
  output2 <- 1 / (1 + exp(-1*(W2 %*% output1 + intercept2)))
  # back propagation
  W2 <- W2 + eta2 * (answer_mat[i,] - output2) %*% t(output1)
  intercept2 <- intercept2 + eta2 * (answer_mat[i,] - output2)
  W1 <- W1 + eta1 * ((output1*(1-output1)) * (t(W2) %*% (answer_mat[i,] - output2))) %*% t(train_mat[i,])
  intercept1 <- intercept1 + eta1 * (output1*(1-output1)) * (t(W2) %*% (answer_mat[i,] - output2))
}
```

出力の計算

```{r}
output_mat1 <- 1 / (1 + exp(-1*(train_mat %*% t(W1) + matrix(1,nrow=28000,ncol=1) %*% matrix(intercept1,ncol=49))))
output_mat2 <- 1 / (1 + exp(-1*(output_mat1 %*% t(W2) + matrix(1,nrow=28000,ncol=1) %*% matrix(intercept2,ncol=10))))
trainres <- max.col(as.matrix(output_mat2)) - 1
```

Confusion Matrix

```{r}
table(trainres,train_label)
```

Categorization Accuracy

```{r}
mean(trainres == train_label)
```

だいぶ精度が上がりました。